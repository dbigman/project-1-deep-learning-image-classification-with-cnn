{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "### Define Loss Function & Optimizer\n",
    "- **Loss Function:**  \n",
    "  Use cross-entropy loss for multi-class classification.\n",
    "- **Optimizer:**  \n",
    "  Choose between:\n",
    "  - Stochastic Gradient Descent (SGD)\n",
    "  - Adam (justify choice based on experimentation or literature).\n",
    "\n",
    "### Set Hyperparameters\n",
    "- **Learning Rate:**  \n",
    "  Start with 0.001 for Adam or 0.01 for SGD.\n",
    "- **Batch Size:**  \n",
    "  Typically 32 or 64.\n",
    "- **Number of Epochs:**  \n",
    "  Start with 50-100 epochs and monitor for early stopping.\n",
    "\n",
    "### Implement Training Loop\n",
    "- **Forward Pass:**  \n",
    "  Compute predictions.\n",
    "- **Backward Pass:**  \n",
    "  Compute gradients and update weights.\n",
    "- **Logging:**  \n",
    "  Track training and validation loss/accuracy.\n",
    "\n",
    "### Early Stopping & Checkpointing\n",
    "- **Early Stopping:**  \n",
    "  Monitor validation loss to halt training when performance plateaus.\n",
    "- **Model Checkpointing:**  \n",
    "  Save the best model based on validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset from TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _parse_raw_image(proto):\n",
    "    \"\"\"Parses a TFRecord example to extract image.\"\"\"\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "    \n",
    "    # Decode the image from raw bytes (Assuming JPEG format, adjust if needed)\n",
    "    image = tf.io.decode_jpeg(parsed_features['image'], channels=3)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Load previously saved TFRecord dataset\n",
    "tfrecord_path = \"balanced_train_20250218_231144.tfrecord\"\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "def inspect_image_shapes(dataset, n=5):\n",
    "    \"\"\"\n",
    "    Inspects n random images from a TFRecord dataset.\n",
    "    If all images have the same shape, returns (image_size, num_channels).\n",
    "    Otherwise, raises an error.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): Parsed TFRecord dataset.\n",
    "        n (int): Number of images to check.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image_size, num_channels) if all images match.\n",
    "        Raises ValueError if shapes are inconsistent.\n",
    "    \"\"\"\n",
    "    image_shapes = set()\n",
    "\n",
    "    # Parse dataset before checking shapes\n",
    "    parsed_dataset = dataset.map(_parse_raw_image)\n",
    "\n",
    "    # Iterate through n images and store their shapes\n",
    "    for image in parsed_dataset.shuffle(1000).take(n):\n",
    "        shape = tuple(image.shape)  # Convert TensorShape to tuple\n",
    "        if len(shape) == 3:  # Ensure shape contains height, width, and channels\n",
    "            image_shapes.add(shape)\n",
    "\n",
    "    # Check if all shapes are identical\n",
    "    if len(image_shapes) == 1:\n",
    "        height, width, num_channels = list(image_shapes)[0]\n",
    "        print(f\"All images have the same shape: ({height}, {width}) with {num_channels} channels.\")\n",
    "        return (height, width), num_channels\n",
    "    else:\n",
    "        raise ValueError(f\"Inconsistent image shapes found: {image_shapes}. Ensure uniform image sizes in TFRecord.\")\n",
    "\n",
    "\n",
    "IMAGE_SIZE, NUM_CHANNELS = inspect_image_shapes(raw_dataset, n = 5)\n",
    "print(f\"Image size: {IMAGE_SIZE}, Channels: {NUM_CHANNELS}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse a TFRecord \n",
    "def _parse_function(proto):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "    \n",
    "    image = tf.io.decode_jpeg(parsed_features['image'], channels=NUM_CHANNELS)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)  # Resize to uniform shape\n",
    "    image = image / 255.0  # Normalize to [0,1]\n",
    "    \n",
    "    label = parsed_features['label']\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# Parse dataset\n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "\n",
    "# Inspect the first few labels\n",
    "for image, label in parsed_dataset.take(5):\n",
    "    print(label.numpy())  # Print label values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "# Get a sorted list of exported model directories matching the naming pattern\n",
    "model_dirs = sorted(glob.glob(\"my_cnn_model_*\"))\n",
    "if not model_dirs:\n",
    "    raise FileNotFoundError(\"No saved model directories found.\")\n",
    "\n",
    "# Select the most recent (latest) model directory\n",
    "latest_model_dir = model_dirs[-1]\n",
    "print(\"Loading the latest model from:\", latest_model_dir)\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model(latest_model_dir)\n",
    "loaded_model.summary()\n",
    "\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function & Optimizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',  # Using 'sparse_categorical_crossentropy' if labels are integers\n",
    "    optimizer=Adam(learning_rate=0.0005),  # You can adjust the learning rate as needed\n",
    "    metrics=['accuracy']  # You can add other metrics like precision and recall if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "\n",
    "# Shuffle and batch the dataset before training\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = parsed_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Is GPU available:\", tf.test.is_built_with_cuda())\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "#Force TensorFlow to Use GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')  # Use only the first GPU\n",
    "        print(\"âœ… GPU is now enabled for TensorFlow\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE  # Uses optimal number of CPU threads\n",
    "\n",
    "# Optimized dataset pipeline for fast GPU training\n",
    "train_dataset = (\n",
    "    parsed_dataset\n",
    "    .shuffle(1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()  # Ensures dataset doesn't run out during multiple epochs\n",
    "    .map(lambda x, y: (x, y), num_parallel_calls=AUTOTUNE)  # Remove unnecessary tensor conversion\n",
    "    .prefetch(AUTOTUNE)  # Keeps GPU utilization high\n",
    ")\n",
    "\n",
    "# Train the model with optimized pipeline\n",
    "model.fit(train_dataset, epochs=10, steps_per_epoch=100)  # Use `steps_per_epoch` to control dataset consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Define dataset size (you need to determine this beforehand)\n",
    "dataset_size = sum(1 for _ in parsed_dataset)  # Count dataset samples\n",
    "\n",
    "# Split dataset (80% train, 20% validation)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# Training dataset\n",
    "train_dataset = (\n",
    "    parsed_dataset.take(train_size)\n",
    "    .shuffle(1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = (\n",
    "    parsed_dataset.skip(train_size)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    # patience = 5,\n",
    "    patience=10, # ytring to increase the \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=100,\n",
    "    validation_steps=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training stopped at epoch: {early_stopping.stopped_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
